---
layout: post
title: Graph 6 - Other GNN : GraphSAGE and GAT
tags: 
  - "graph" 
  - "tech"
---

## GraphSAGE
前面介紹到把neighbor embedding以自訂的方式(前面用mean舉裡)聚合起來，有人提出一個更彈性的作法，把原本的Wh+Bh的形式改成用一個自訂的AGG function來聚合neighbor，然後用concate的方式接上Bh_v，這樣帶來的好處是把neighbor當成是一個獨立的matrix來看待，除了可以用mean外，作者也提出用pool或LSTM來當AGG。
- **Normal GCN**
![](https://i.imgur.com/hNr9Pbh.png){: width="400" }  
- **GraphSAGE**
![](https://i.imgur.com/CCQoh6f.png){: width="500" }  

GraphSAGE 不同於原本GNN是W*h得到msg後再AGG，GraphSAGE是先AGG所有鄰居的h後跟自己的h concate再乘以W，然後丟進activation function。
- AGG原作者也提出了幾種 :  Mean,Pool,LSTM
- Pool是多塞一層MLP作為transformation再取mean
- LSTM則是Reshuffle後依序丟進LSTM
- 在丟出每層的h之前會再做一個L2-norm(除以 平方和開根號)，在某些情況下效果會比較好

**Sum**
![](https://i.imgur.com/BzTAb1T.png)
**Pool**
![](https://i.imgur.com/m8vntBp.png)
**LSTM**
![](https://i.imgur.com/pUynXqe.png)

## GAT (Graph Attention Network)
繼GCN、GraphSAGE後，出現了新的GNN種類，GAT 新增一個alpha作為可學習的Attention weight，sigma(alpha_vu * W * h)
![](https://i.imgur.com/bVS5wsA.png)
公式跟前面兩個不同地方在把除以鄰居數N(v)改成乘以alpha_vu，原本的除以鄰居數是默認每個鄰居一樣重要，GAT認為有些鄰居對node的影響較大，alpha是用來學習哪些鄰居比較重要的。

**Q :  這個alpha要怎麼訂？**  

![](https://i.imgur.com/OREdetr.png)
定義一個變數e_vu = a(msg_u,msg_v) 代表對node v來說node u的msg的重要性，有了e，可以有很多方法得到alpha，最常用的是softmax，因為可以把機率normalize成總和為1(p28)
![](https://i.imgur.com/0DLvznZ.png)
![](https://i.imgur.com/hf4hwyU.png)
 
**Q :  那function a是哪來的？**  

a是attention mechanism function，可以用single layer neural network，因為是NN，可以有trainable weight。這個a與整個GAT一起train就行了，一起更新。

### Multi-head attention
有一個穩定attention的方法叫Multi-head attention
就是用不同的參數設定train出好幾個attention score，然後用mean, sum或自訂的方法當做AGG算下一輪的attention score，雖然能讓score更穩定，但要多train，需要取捨。

### Attention的好處
1. 把鄰居的重要性也考慮進去
2. 運算有效率，算attention時可以平行化
3. 參數固定，空間是夠用的