---
layout: post
title: Graph 6 - Other GNN : GraphSAGE and GAT
tags: 
  - "graph" 
  - "tech"
---

## GraphSAGE
前面介紹到把neighbor embedding以自訂的方式(前面用mean舉裡)聚合起來，有人提出一個更彈性的作法，把原本的Wh+Bh的形式改成用一個自訂的AGG function來聚合neighbor，然後用concate的方式接上Bh_v，這樣帶來的好處是把neighbor當成是一個獨立的matrix來看待，除了可以用mean外，作者也提出用pool或LSTM來當AGG。
- **Normal GCN**
![](https://i.imgur.com/hNr9Pbh.png){: width="400" }  
- **GraphSAGE**
![](https://i.imgur.com/CCQoh6f.png){: width="500" }  

GraphSAGE 不同於原本GNN是W*h得到msg後再AGG，GraphSAGE是先AGG所有鄰居的h後跟自己的h concate再乘以W，然後丟進activation function。
- AGG原作者也提出了幾種 Mean,Pool,LSTM
- Pool是多塞一層MLP作為transformation再取mean
- LSTM則是Reshuffle後依序丟進LSTM
- 在丟出每層的h之前會再做一個L2-norm(除以 平方和開根號)，在某些情況下效果會比較好

**Sum**
![](https://i.imgur.com/BzTAb1T.png)
**Pool**
![](https://i.imgur.com/m8vntBp.png)
**LSTM**
![](https://i.imgur.com/pUynXqe.png)

## GAT (Graph Attention Network)
繼GCN、GraphSAGE後，出現了新的GNN種類，GAT 新增一個alpha作為可學習的Attention weight，sigma(alpha_vu * W * h)
![](https://i.imgur.com/bVS5wsA.png)
公式跟前面兩個不同地方在把除以鄰居數N(v)改成乘以alpha_vu，原本的除以鄰居數是默認每個鄰居一樣重要，GAT認為有些鄰居對node的影響較大，alpha是用來學習哪些鄰居比較重要的。
