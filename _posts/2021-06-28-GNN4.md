---
layout: post
title: Graph 8 - GNN Prediction
tags: 
  - "graph" 
  - "tech"
---

前面著重在建構GNN會遇到的問題，這篇把重點放在GNN效果的相關議題。
整個GNN的建立過程如下  
**Input Graph -> Graph Neural Network -> Node Embeddings -> Prediction Head -> Prediction -> Loss function and Evaluation metric**

## Prediction Head 
把Embedding結果跟Prediction串起來的部分，不同task會有不同的Prediction Head，例如GNN的output會是一個h_v(L)，透過Prediction Head可以把它轉成機率，可以根據不同特徵定義Prediction Head 
### Node-Level task 
用node embedding做prediction，Prediction Head = W(H)，是一個矩陣，乘以Embedding h得到一個機率分佈。
![](https://i.imgur.com/6TyMRAu.png){: width="500" }
### Edge-Level task 
用pair of node embedding做prediction，Prediction Head = Head_edge(h1,h2)，input是兩個embedding，output是他們之間連接的機率。
- 方法一 : 可以把hu跟hv接起來，然後通過一個Linear function(EX MLP) -> Linear(Concate(h_u,h_v))
- 方法二 : Dot product -> (h_u)^T * hv ，也可以做到k-way prediction上，直接做k次 ->  (h_u)^T * W^(k) * h_v

![](https://i.imgur.com/lOKhF2u.png){: width="300" }

### Graph-Level task 
用整張Graph做prediction，拿到每個node的embedding，可以做
1. Global mean pooling
![](https://i.imgur.com/Vskqy2a.png){: width="300" }

2. Global max pooling
![](https://i.imgur.com/QhALnRt.png){: width="300" }

3. Global sum pooling
![](https://i.imgur.com/uBBgSNj.png){: width="300" }

以上方法在小圖還可以，大圖上就效果不好了，除此之外也有很多種狀況不適合 例如：  
兩張圖的embedding分別是{-1,-2,0,2,1} {-10,-20,0,10,20}，在Sum的情況下都會等於0，但本質上特徵差很多，所以有一種簡單的做法是hierarchically aggregate並且加上非線性函數 
![](https://i.imgur.com/VFTVaHw.png){: width="500" }

4. Hierarchical Pooling
每一組一組的subgraph先pooling，在對這些結果分成一群一群再二次pooling 
![](https://i.imgur.com/tsd7zLq.png){: width="500" }

## Prediction
- Supervised : label from external sources (groud true來自圖以外)
- Unsupervised : Sinals come from graphs 例如 link prediction 把某個edge遮起來去訓練判斷有沒有連線，沒有靠額外的外部資訊，label來自圖本身，link prediction看起來跟一般認知的supervised任務也有點像，因為是有label學習的，所以也稱這種的是**self-supervised**

根據label又可以分成**Node label**, **Edge label**, **Graph label**
- Node label : Node的分類
- Edge label : 是否有連線
- Graph label : 兩圖是否isomorphic

## Loss Function and Evaluation Metric
這部分與一般NN差不多
- Loss Function : Cross Entropy, MSE ....
- Evaluation metric : acc, recall, f1, roc, auc ....

## Dataset Split
對於拿到的一張Graph應該如何把他切成train, val, test 三個部分，圖的切法不像傳統ML的切法這麼簡單，因為圖資料彼此都不是獨立的，關於切的次數也有分以下兩種 : 
- Fixed split : 一次把三個部分切出來
- Random split : 每次隨機切，最後回報avg performance

### 解法一 : Transductive setting
- embedding用整張圖來train，label才分開用train,val,test(把部分label遮起來)
- 適用於node, edge label task
![](https://i.imgur.com/qi3k5qA.png){: width="500" }

### 解法二 Inductive setting
- 直接切開，分開train,val,test，把他們之間的edge給刪掉
- 分別算各自的Embedding，然後用各自有的label算Loss
- 適用於node, edge, graph label task
![](https://i.imgur.com/rcBMeG5.png){: width="500" }

## Example : Link Prediction
![](https://i.imgur.com/nsssSUN.png){: width="500" }

### Inductive的作法
典型的Self-supervised任務，因為要的label都直接在圖結構裡了，在這種任務中edge可以分成兩種 : 
- Message edges : 用來傳遞msg的edge
- Supervision edges : 用來當作label算loss的edge，不會做msg的傳遞，embedding階段當作不存在(不會進入GNN視野)

dataset split可以分成兩階段 : 
- 階段一 : 將所有的edge分成msg edge和supervision 
![](https://i.imgur.com/wxBkQCC.png){: width="500" }
- 階段二 : 把整張圖再分成train, val, test
![](https://i.imgur.com/domHVUq.png){: width="500" }
- 所以三個subgraph裡面都各自會有自己的兩種edge

### Transductive
大部分的人在做link prediction的默認做法，在train時一樣分成msg edge和supervision edge，然後僅對supervision edge做切割。以下實線為training msg edges，虛線為supervision edges

![](https://i.imgur.com/m6ASUZ3.png)
- 在train階段，會拿training msg edges訓練，training supervision edges測試。
- 在val階段，會拿training msg edges, training supervision edges訓練，validation edges測試。
- 在test階段，會拿training msg edges, training supervision edges, validation edges訓練，test edges測試。